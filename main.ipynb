{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install torch requires python 3.8.6 using below nightly build\n",
    "pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azure OpenAI limits the max input size 1.\n",
    "embeddings = OpenAIEmbeddings(engine=<your deployment name>, chunk_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using embedded DuckDB with persistence: data will be stored in: C:\\Users\\qianwens\\gpt\\db\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from langchain import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "OPENAI_API_KEY= \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2022-12-01\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://qianwenai.openai.azure.com\"\n",
    "\n",
    "persist_directory = \"C:\\\\Users\\\\qianwens\\\\gpt\\\\db\"\n",
    "embeddings = OpenAIEmbeddings(chunk_size=1)\n",
    "if(not os.path.exists(persist_directory)):\n",
    "    loader = UnstructuredMarkdownLoader(\"C:\\\\Users\\\\qianwens\\\\tsg.md\")\n",
    "    data = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(data)\n",
    "    db = Chroma.from_documents(docs, embeddings, persist_directory=persist_directory)   \n",
    "    db.persist()\n",
    "else:\n",
    "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! engine is not default parameter.\n",
      "                    engine was transferred to model_kwargs.\n",
      "                    Please confirm that engine is what you intended.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI(\n",
    "model_name=\"gpt-35-turbo\",\n",
    "temperature=0,\n",
    "model_kwargs={\"api_key\": OPENAI_API_KEY, \"api_base\": \"https://qianwenai.openai.azure.com\", \"api_type\": \"azure\",\n",
    "                                \"api_version\": \"2023-03-15-preview\", },\n",
    "max_tokens=512,\n",
    "verbose=True,\n",
    "engine=\"gpt-35-turbo\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I'm sorry, I don't understand your question. Can you please provide more context or clarify your inquiry?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "You can use the following command to reboot a node in AKS:\n",
      "\n",
      "kubectl drain <node_name> --ignore-daemonsets && kubectl delete node <node_name>\n",
      "\n",
      "This will gracefully evict all the pods running on the node and then delete the node. The node will then be recreated by AKS with a fresh OS image.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "mysql_template = \"\"\"As a service connector on azure tsg bot, your goal is to provide accurate\n",
    "and helpful information about service connector on azure. You should answer user inquiries based on the\n",
    "context provided and avoid making up answers. If you don't know the answer,\n",
    "simply state that you don't know.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "template=mysql_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "bot = RetrievalQA.from_chain_type(\n",
    "llm=llm,\n",
    "chain_type=\"stuff\",\n",
    "retriever=db.as_retriever(),\n",
    "chain_type_kwargs={\"prompt\": prompt},\n",
    "verbose=True,\n",
    ")\n",
    "\n",
    "while (True):\n",
    "    query = input(\"Enter query: \")\n",
    "    if query == \"exit\":\n",
    "        break\n",
    "    result = bot.run(query)\n",
    "    print(result)\n",
    "\n",
    "db = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
